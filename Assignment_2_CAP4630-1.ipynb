{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNLUDKg_oIJy",
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Assignment 2: Logistic Regression and Classification Error Metrics\n",
    "\n",
    "# CAP 4630: Intro Artificial Intelligence\n",
    "\n",
    "**Student's name:** Annaly Rocha\n",
    "\n",
    "**Section:** 001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-k36VIXoIJz",
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "You will be using the Human Activity Recognition with Smartphones dataset, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n",
    "\n",
    "The dataset is already provided on canvas. So, you can download the dataset from canvas. To know more information about the features of the dataset, you can take a look at the website: [Human Activity Recognition with Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones).\n",
    "\n",
    "For each record in the dataset it is provided:\n",
    "\n",
    "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n",
    "- Triaxial Angular velocity from the gyroscope.\n",
    "- A 561-feature vector with time and frequency domain variables.\n",
    "- Its activity label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TF6MeCZtoIJ0",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "#Data Path has to be set as per the file location in your system\n",
    "#data_path = ['..', 'data']\n",
    "data_path = ['data']\n",
    "\n",
    "# Ignore the warning\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v-wS4jEOGMAD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df9iX9zLoIJ0",
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 1 (10 points)\n",
    "\n",
    "Import the data and do the following:\n",
    "\n",
    "* Read the dataset\n",
    "* By looking at the dataset, do you think the floating point values need to be scaled? Just write your answer (no need to write code for this)\n",
    "* Examine the data types--there are many columns, so it might be wise to use value counts\n",
    "\n",
    "* Split the dataset into X_data and y_data\n",
    "* Determine the breakdown of each activity\n",
    "* Encode the activity label as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HiKZU9xIoIJ1",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded!\n",
      "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
      "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
      "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
      "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
      "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
      "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
      "\n",
      "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
      "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
      "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
      "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
      "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
      "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
      "\n",
      "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
      "0         -0.923527         -0.934724  ...                        -0.710304   \n",
      "1         -0.957686         -0.943068  ...                        -0.861499   \n",
      "2         -0.977469         -0.938692  ...                        -0.760104   \n",
      "3         -0.989302         -0.938692  ...                        -0.482845   \n",
      "4         -0.990441         -0.942469  ...                        -0.699205   \n",
      "\n",
      "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
      "0                    -0.112754                              0.030400   \n",
      "1                     0.053477                             -0.007435   \n",
      "2                    -0.118559                              0.177899   \n",
      "3                    -0.036788                             -0.012892   \n",
      "4                     0.123320                              0.122542   \n",
      "\n",
      "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
      "0                         -0.464761                             -0.018446   \n",
      "1                         -0.732626                              0.703511   \n",
      "2                          0.100699                              0.808529   \n",
      "3                          0.640011                             -0.485366   \n",
      "4                          0.693578                             -0.615971   \n",
      "\n",
      "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  subject  \\\n",
      "0             -0.841247              0.179941             -0.058627        1   \n",
      "1             -0.844788              0.180289             -0.054317        1   \n",
      "2             -0.848933              0.180637             -0.049118        1   \n",
      "3             -0.848649              0.181935             -0.047663        1   \n",
      "4             -0.847865              0.185151             -0.043892        1   \n",
      "\n",
      "   Activity  \n",
      "0  STANDING  \n",
      "1  STANDING  \n",
      "2  STANDING  \n",
      "3  STANDING  \n",
      "4  STANDING  \n",
      "\n",
      "[5 rows x 563 columns]\n"
     ]
    }
   ],
   "source": [
    "#write code to read the dataset\n",
    "data = pd.read_csv('Human_Activity_Recognition_Using_Smartphones_Data.csv')\n",
    "print(\"Dataset successfully loaded!\")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx2NYYQ6oIJ1",
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "By looking at the dataset, do you think the floating point values need to be scaled? Just write your answer (no need to write code for this).\n",
    "\n",
    "Your answer: Yes, because there are different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9QJnj6FYoIJ1",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for Activity:\n",
      "Activity\n",
      "LAYING                1407\n",
      "STANDING              1374\n",
      "SITTING               1286\n",
      "WALKING               1226\n",
      "WALKING_UPSTAIRS      1073\n",
      "WALKING_DOWNSTAIRS     986\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Write code to examine the data types--there are many columns, so it might be wise to use value counts\n",
    "for column in data.select_dtypes(include=['object', 'category']).columns:\n",
    "    print(f\"\\nValue counts for {column}:\")\n",
    "    print(data[column].value_counts())\n",
    "if 'activity' in data.columns:\n",
    "    y_column = 'activity'\n",
    "else:\n",
    "    # Try to identify the target column (often the last column)\n",
    "    y_column = data.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FcaILMF1oIJ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (7352, 562)\n",
      "Target shape: (7352,)\n"
     ]
    }
   ],
   "source": [
    "# Write code to split the dataset into X_data and y_data\n",
    "X_data = data.drop(y_column, axis=1)\n",
    "y_data = data[y_column]\n",
    "\n",
    "print(\"Features shape:\", X_data.shape)\n",
    "print(\"Target shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "o79ygysZoIJ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Breakdown of activities:\n",
      "Activity\n",
      "LAYING                1407\n",
      "STANDING              1374\n",
      "SITTING               1286\n",
      "WALKING               1226\n",
      "WALKING_UPSTAIRS      1073\n",
      "WALKING_DOWNSTAIRS     986\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Write code to examine the breakdown of activities.\n",
    "print(\"\\nBreakdown of activities:\")\n",
    "activity_counts = y_data.value_counts()\n",
    "print(activity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kFJfwRmnoIJ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activity encoding mapping:\n",
      "LAYING -> 0\n",
      "SITTING -> 1\n",
      "STANDING -> 2\n",
      "WALKING -> 3\n",
      "WALKING_DOWNSTAIRS -> 4\n",
      "WALKING_UPSTAIRS -> 5\n",
      "\n",
      "First 10 encoded activity labels:\n",
      "[2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#Write code to Encode the activity label as an integer. Use `LabelEncoder` to fit_transform the \"Activity\" column.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_data)\n",
    "\n",
    "# Display the mapping between original labels and encoded integers\n",
    "print(\"\\nActivity encoding mapping:\")\n",
    "for i, activity in enumerate(label_encoder.classes_):\n",
    "    print(f\"{activity} -> {i}\")\n",
    "\n",
    "print(\"\\nFirst 10 encoded activity labels:\")\n",
    "print(y_encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWYXWOE9oIJ1"
   },
   "source": [
    "## Question 2 (5 points)\n",
    "\n",
    "When we train a machine learning model, we usually divide the dataset into training and testing sets.\n",
    "However, if our dataset contains multiple activity classes (for example, Walking, Sitting, Standing, Laying, Walking Upstairs, Walking Downstairs), it’s important that each class appears in the same proportion in both sets.\n",
    "\n",
    "For instance, if 20% of your entire dataset represents Sitting, you don’t want a situation where only 5% of the test data represents Sitting, this would make your test results unreliable because the distribution of activities has changed.\n",
    "\n",
    "That’s where Stratified Sampling comes in.\n",
    "Scikit-learn’s StratifiedShuffleSplit helps us split the data while maintaining the same class distribution (ratio) in both training and test sets. It randomly shuffles the data, but ensures that each class keeps the same proportion as in the original dataset.\n",
    "\n",
    "Why Use StratifiedShuffleSplit?\n",
    "*   Prevents bias toward overrepresented classes in training or testing.\n",
    "*   Ensures consistent and fair evaluation across all activity classes.\n",
    "*   Especially useful for multi-class classification problems like Human Activity Recognition.\n",
    "\n",
    "Your task is to split your Human Activity Recognition dataset into training and testing sets. You may use any method (e.g., train_test_split), but to ensure the same ratio of activity classes in both sets, apply Scikit-learn’s StratifiedShuffleSplit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2h6TROc9oIJ1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Get the split indexes\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "train_index, test_index = next(sss.split(X_data, y_data))\n",
    "\n",
    "#write code here\n",
    "X_train = X_data.iloc[train_index]\n",
    "y_train = y_data.iloc[train_index]\n",
    "\n",
    "X_test = X_data.iloc[test_index]\n",
    "y_test = y_data.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcqy2pO79-vC"
   },
   "source": [
    "* Regardless of methods used to split the data, you can compare the ratio of classes in both the train and test splits if you run the below cells (you dont need to write any code just observe the usefulness of using StratifiedShuffleSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vqvjODKXoIJ2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Activity\n",
       "LAYING                0.191411\n",
       "STANDING              0.186941\n",
       "SITTING               0.174893\n",
       "WALKING               0.166731\n",
       "WALKING_UPSTAIRS      0.145939\n",
       "WALKING_DOWNSTAIRS    0.134085\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no need to modify anything in this cell. just run and see the outcome\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ytUkwjUAoIJ2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Activity\n",
       "LAYING                0.191296\n",
       "STANDING              0.186763\n",
       "SITTING               0.174977\n",
       "WALKING               0.166818\n",
       "WALKING_UPSTAIRS      0.145966\n",
       "WALKING_DOWNSTAIRS    0.134180\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no need to modify anything in this cell. just run and see the outcome\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZumT4xSoIJ2"
   },
   "source": [
    "## Question 3 (10 points)\n",
    "\n",
    "* Fit a logistic regression model without any regularization using all of the features, and perform model prediction.\n",
    "* Next, use cross validation to determine the hyperparameters, fit models using L2 regularization, and perform model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R7DQ5yLtoIJ2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Write code to fit a logistic regression model without any regularization using all of the features and perform prediction.\n",
    "\n",
    "model_no = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000, random_state=42)\n",
    "model_no.fit(X_train, y_train)\n",
    "y_pred_no = model_no.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BtxtqgOVoIJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9796010879419764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#write code to calculate accuracy score for model without regularization\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IEZ8M6R6oIJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#Write code to use cross validation to determine the hyperparameters, fit models using L2 regularization and perform prediction.\n",
    "model_l2 = LogisticRegressionCV(\n",
    "    Cs = 5,\n",
    "    cv = 3,\n",
    "    penalty = 'l2',\n",
    "    solver = 'lbfgs',\n",
    "    max_iter = 500,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "model_l2.fit(X_train, y_train)\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "\n",
    "print(\"Best C:\", model_l2.C_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtaDaHVtoIJ3"
   },
   "source": [
    "## Question 4 (5 points)\n",
    "\n",
    "For each model (one without regularization and one with regularization), calculate the following error metrics:\n",
    "\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FUgQmDBpOlJF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model without Regularization:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            LAYING     1.0000    1.0000    1.0000       422\n",
      "           SITTING     0.9481    0.9456    0.9468       386\n",
      "          STANDING     0.9469    0.9515    0.9492       412\n",
      "           WALKING     1.0000    1.0000    1.0000       368\n",
      "WALKING_DOWNSTAIRS     0.9933    0.9966    0.9949       296\n",
      "  WALKING_UPSTAIRS     0.9969    0.9907    0.9938       322\n",
      "\n",
      "          accuracy                         0.9796      2206\n",
      "         macro avg     0.9808    0.9807    0.9808      2206\n",
      "      weighted avg     0.9796    0.9796    0.9796      2206\n",
      "\n",
      "Accuracy: 0.9796010879419764\n",
      "\n",
      "Model with L2 Regularization (CV):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            LAYING     1.0000    1.0000    1.0000       422\n",
      "           SITTING     0.9635    0.9585    0.9610       386\n",
      "          STANDING     0.9614    0.9660    0.9637       412\n",
      "           WALKING     0.9973    1.0000    0.9986       368\n",
      "WALKING_DOWNSTAIRS     1.0000    0.9966    0.9983       296\n",
      "  WALKING_UPSTAIRS     0.9969    0.9969    0.9969       322\n",
      "\n",
      "          accuracy                         0.9855      2206\n",
      "         macro avg     0.9865    0.9863    0.9864      2206\n",
      "      weighted avg     0.9855    0.9855    0.9855      2206\n",
      "\n",
      "Accuracy: 0.985494106980961\n",
      "Best C for L2-regularized model: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Write code\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Model without Regularization:\")\n",
    "report_no = classification_report(y_test, y_pred_no, digits=4)\n",
    "accuracy_no = accuracy_score(y_test, y_pred_no)\n",
    "print(report_no)\n",
    "print(\"Accuracy:\", accuracy_no)\n",
    "\n",
    "print(\"\\nModel with L2 Regularization (CV):\")\n",
    "report_l2 = classification_report(y_test, y_pred_l2, digits=4)\n",
    "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "print(report_l2)\n",
    "print(\"Accuracy:\", accuracy_l2)\n",
    "\n",
    "print(\"Best C for L2-regularized model:\", model_l2.C_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjIRgurXJAgH"
   },
   "source": [
    "## Question 5 Confusion Matrix Construction (5 points)\n",
    "\n",
    "You trained a model to classify six human activities.\n",
    "\n",
    "Write code to generate the confusion matrix for your trained model on the test set.\n",
    "\n",
    "Identify which activities are most frequently confused. Provide one possible explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uzNgsjvEoIJ3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                    LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
      "LAYING                 422        0         0        0                   0   \n",
      "SITTING                  0      365        21        0                   0   \n",
      "STANDING                 0       20       392        0                   0   \n",
      "WALKING                  0        0         0      368                   0   \n",
      "WALKING_DOWNSTAIRS       0        0         0        0                 295   \n",
      "WALKING_UPSTAIRS         0        0         1        0                   2   \n",
      "\n",
      "                    WALKING_UPSTAIRS  \n",
      "LAYING                             0  \n",
      "SITTING                            0  \n",
      "STANDING                           0  \n",
      "WALKING                            0  \n",
      "WALKING_DOWNSTAIRS                 1  \n",
      "WALKING_UPSTAIRS                 319  \n"
     ]
    }
   ],
   "source": [
    "#Write code here\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_no)\n",
    "cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The model most often confused 'SITTING' with 'STANDING'.\n"
     ]
    }
   ],
   "source": [
    "#b\n",
    "cm_copy = cm.copy()\n",
    "\n",
    "np.fill_diagonal(cm_copy, 0)\n",
    "most_confused_index = np.unravel_index(np.argmax(cm_copy), cm_copy.shape)\n",
    "true_activity = label_encoder.classes_[most_confused_index[0]]\n",
    "predicted_activity = label_encoder.classes_[most_confused_index[1]]\n",
    "print(f\"\\nThe model most often confused '{true_activity}' with '{predicted_activity}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "c. If a class has high precision but low recall, the model is usually right when it predicts it, but it misses many real cases. If a class has high recall but low precision, the model finds most real cases, but also makes more mistakes by labeling other activities as that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V6mXL_BG1v9"
   },
   "source": [
    "## Question 6: Precision, Recall, and F1-Score (10 points)\n",
    "\n",
    "a. Write code to compute precision, recall, and F1-score for each activity class.\n",
    "\n",
    "b. Identify one class with high precision but low recall and one with high recall but low precision.\n",
    "\n",
    "c. Interpret what these results mean in terms of your classifier’s behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "d-3ajuaNKMdF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Activity  Precision    Recall  F1-Score  Support\n",
      "0              LAYING   1.000000  1.000000  1.000000      422\n",
      "1             SITTING   0.948052  0.945596  0.946822      386\n",
      "2            STANDING   0.946860  0.951456  0.949153      412\n",
      "3             WALKING   1.000000  1.000000  1.000000      368\n",
      "4  WALKING_DOWNSTAIRS   0.993266  0.996622  0.994941      296\n",
      "5    WALKING_UPSTAIRS   0.996875  0.990683  0.993769      322\n"
     ]
    }
   ],
   "source": [
    "#write code\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, \n",
    "    y_pred_no, \n",
    "    average=None, \n",
    "    labels=label_encoder.classes_\n",
    ")\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Activity': label_encoder.classes_,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classes with HIGH PRECISION but LOW RECALL:\n",
      "Empty DataFrame\n",
      "Columns: [Activity, Precision, Recall, F1-Score, Support]\n",
      "Index: []\n",
      "\n",
      "Classes with HIGH RECALL but LOW PRECISION:\n",
      "Empty DataFrame\n",
      "Columns: [Activity, Precision, Recall, F1-Score, Support]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "high_precision_low_recall = results_df.loc[\n",
    "    (results_df['Precision'] > 0.98) & (results_df['Recall'] < 0.95)\n",
    "]\n",
    "\n",
    "print(\"\\nClasses with HIGH PRECISION but LOW RECALL:\")\n",
    "print(high_precision_low_recall)\n",
    "\n",
    "high_recall_low_precision = results_df.loc[\n",
    "    (results_df['Recall'] > 0.98) & (results_df['Precision'] < 0.95)\n",
    "]\n",
    "\n",
    "print(\"\\nClasses with HIGH RECALL but LOW PRECISION:\")\n",
    "print(high_recall_low_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmygKR2mKnmg"
   },
   "source": [
    "# **Write explanation for Question 5b and 5c.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2xuZFuzKzL-"
   },
   "source": [
    "##\n",
    "The confusion matrix shows that the model predicts most activities very accurately. The activity SITTING is most often confused with STANDING, likely because their movements are very similar and hard for the sensors to tell apart."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
